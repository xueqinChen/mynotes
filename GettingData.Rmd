---
output:
  html_document: default
  word_document:
    highlight: tango
---
# Lesson1

## Local File

```
if (!file.exists("data")) {
    dir.create("data")
}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "cameras.csv", method = "curl")
dateDownloaded <- date()

cameraData <- read.csv("./data/cameras.csv")
cameraData <- read.table("./data/cameras.csv", sep = ",", header = TRUE)
head(cameraData)
```

In my experience, the biggest trouble with reading flat files are quotation marks ` or " placed in data values, setting quote="" often resolves these.


## Excel

### Download the file to load

```
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.xlsx?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.xlsx",method="curl")
dateDownloaded <- date()
```
### read xlsx
```
library(xlsx)
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx",sheetIndex=1,
                              colIndex=colIndex,rowIndex=rowIndex)
head(cameraData)
```



## XML

### Read the file into R

```
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
```

### Directly access parts of the XML document

```
rootNode[[1]]
rootNode[[1]][[1]]
```

### Programatically extract parts of the file

```
xmlSApply(rootNode,xmlValue)
```

### XPath
* /node Top level node
* //node Node at any level
* node[@attr-name] Node with an attribute name
* node[@attr-name='bob'] Node with attribute name attr-name='bob'

```
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
```

### Extract content by attributes

```
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
```


## Json

### Reading data from JSON
* library(jsonlite)
* jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
* names(jsonData)
* jsonData$name
* names(jsonData$owner)
* jsonData$owner$login

### Writing data frames to JSON
* myjson <- toJSON(iris, pretty=TRUE)
* cat(myjson)

### Convert back to JSON
* iris2 <- fromJSON(myjson)
* head(iris2)

### resource
* http://www.json.org/
* A good tutorial on jsonlite - http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/


## Data Type

### Create data tables just like data frames
```
library(data.table)
DF = data.frame(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
## See all the data tables in memory
tables()
```

### Subsetting rows

```
DT[2,]
DT[DT$y=="a",]
DT[c(2,3)]
```

### Subsetting column

### Calculating values for variables with expressions
```
DT[,list(mean(x),sum(z))]
DT[,table(y)]
```

### Adding new columns

DT[,w:=z^2]

copy

### Multiple operations

```
DT[,m:= {tmp <- (x+z); log2(tmp+5)}]
DT[,a:=x>0]
DT[,b:= mean(x+w),by=a]
```

### Special variables

```
set.seed(123);
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
```

### Keys
```
DT <- data.table(x=rep(c("a","b","c"),each=100), y=rnorm(300))
setkey(DT, x)
DT['a']
```

### Joins

```
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
```

### Fast reading

```
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE, sep="\t"))
```

# Lesson2
 
## readSQL

On a Mac: install.packages("RMySQL")

### Connecting and listing databases

* ucscDb <- dbConnect(MySQL(),user="genome", host="genome-mysql.cse.ucsc.edu") result <- dbGetQuery(ucscDb,"show databases;");
* dbDisconnect(ucscDb);


### Connecting to hg19 and listing tables

```
 hg19 <- dbConnect(MySQL(),user="genome", db="hg19",
                    host="genome-mysql.cse.ucsc.edu")
 allTables <- dbListTables(hg19)
 length(allTables)

 allTables[1:5]
```

### Get dimensions of a specific table

```
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```

### Read from the table

```
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```

### Select a specific subset

query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query);
quantile(affyMis$misMatches)

affyMisSmall <- fetch(query,n=10); dbClearResult(query);
dim(affyMisSmall)



## HDF5
R HDF5 package

### Install
```
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")

library(rhdf5)
created = h5createFile("example.h5")
created
```

### Create groups

```
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
```

### Write to groups

```
A = matrix(1:10,nr=5,nc=2)
h5write(A, "example.h5","foo/A")
B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5","foo/foobaa/B")
h5ls("example.h5")
```

### Write a data set

```
df = data.frame(1L:5L,seq(0,1,length.out=5),
  c("ab","cde","fghi","a","s"), stringsAsFactors=FALSE)
h5write(df, "example.h5","df")
h5ls("example.h5")
```

### Reading data

```
readA = h5read("example.h5","foo/A")
readB = h5read("example.h5","foo/foobaa/B")
readdf= h5read("example.h5","df")
```

### Writing and reading chunks

```
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1))
h5read("example.h5","foo/A")
```

## readingFromWeb

### Getting data off webpages - readLines()
```
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
```

### Parsing with XML
```
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)

xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
```

### GET from the httr package

```
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
```


# Lesson3

## Summarizing Data
plyr package:http://site.douban.com/182577/widget/notes/10567181/note/246634257/

* Make summary 
1. summary(restData)

* Quantiles of quantitative variables
1. quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))

* Make table(counts at each combination of factor levels)
1. table(restData$zipCode,useNA="ifany")
2. table(restData$councilDistrict,restData$zipCode)

* Check for missing values
1. sum(is.na(restData$councilDistrict))

* Row and column sums
1. colSums(is.na(restData))
2. all(colSums(is.na(restData))==0)

* Values with specific characteristics(count)
1. table(restData$zipCode %in% c("21212"))
2. table(restData$zipCode %in% c("21212","21213"))
3. restData[restData$zipCode %in% c("21212","21213"),]

* Cross tabs
1. xt <- xtabs(Freq ~ Gender + Admit,data=DF)(Among var1 in Gender and var2 in Admit cols, calculate the Freq)

* Flat tabs(human readable)
1. warpbreaks$replicate <- rep(1:9, len = 54)
2. xt = xtabs(breaks ~.,data=warpbreaks)
3. ftable(xt)

* Size of Data Set
```
fakeData = rnorm(1e5)
object.size(fakeData)
print(object.size(fakeData),units="Mb")
```

## Creating New Variables

* Creating sequences
```
s1 <- seq(1,10,by=2) ; s1
s2 <- seq(1,10,length=3); s2
x <- c(1,3,8,25,100); seq(along = x)
y <- rep(1:9, len=54)
```

* Subsetting variables
```
restData$nearMe = restData$neighborhood %in% c("Roland Park", "Homeland")
table(restData$nearMe)
```

* Creating binary variables
```
y <- rep(1:9, len=18)
tmp <- ifelse(y<2, TRUE, FALSE)
table(tmp, y<2)
```

* Cut
```
# cut divides the range of x into intervals
restData$zipGroups = cut(restData$zipCode,breaks=quantile(restData$zipCode))
table(restData$zipGroups)
```

* Creating factor variables
```
restData$zcf <- factor(restData$zipCode)
restData$zcf[1:10]
```

* Levels of factor variables
```
yesno <- sample(c("yes","no"),size=10,replace=TRUE)
yesnofac = factor(yesno,levels=c("yes","no"))
relevel(yesnofac,ref="no") 
# no is the first level
```

* Common transforms
```
abs(x) absolute value
sqrt(x) square root
ceiling(x) ceiling(3.475) is 4
floor(x) floor(3.475) is 3
round(x,digits=n) round(3.475,digits=2) is 3.48
signif(x,digits=n) signif(3.475,digits=2) is 3.5
cos(x), sin(x) etc.
log(x) natural logarithm
log2(x), log10(x) other common logs
exp(x) exponentiating x
```

## Reshaping Data

* Start with reshaping
```
library(reshape2)
head(mtcars)
```

* Melting data frames
```
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
head(carMelt,n=3)
tail(carMelt,n=3)
```

* Casting data frames
```
cylData <- dcast(carMelt, cyl ~ variable)
cylData
cylData <- dcast(carMelt, cyl ~ variable,mean)
cylData
```

* Averaging values
```
head(InsectSprays)
# the four way are the same result
#--1
tapply(InsectSprays$count,InsectSprays$spray,sum)
#--2
spIns =  split(InsectSprays$count,InsectSprays$spray)
sprCount = lapply(spIns,sum)
unlist(sprCount)
#--3
spIns =  split(InsectSprays$count,InsectSprays$spray)
sapply(spIns,sum)
#--4
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
```

* Creating a new variable
```
spraySums <- ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
dim(spraySums)
head(spraySums)
```
```
# for total data, output one number
avg = mean(InsectSprays$count, na.rm = TRUE)
# for every data, output length(InsectSprays$count) number
sum=ave(InsectSprays$count,FUN=mean)
```


## Editing Text Variables

tolower()
toupper()
strsplit(): strsplit(value,split)
eg: strsplit(names(cameraData),"\\.")
sapply(): sapply(list,function) 
eg: firstElement <- function(x){x[1]}
    sapply(splitNames,firstElement)

## Date

please see the “Exploratory_Data_Analysis_practice”











